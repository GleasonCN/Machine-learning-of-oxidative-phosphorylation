import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,
                             recall_score, confusion_matrix, roc_curve, precision_score, matthews_corrcoef)
from sklearn.preprocessing import StandardScaler

# Function to plot confusion matrix
def plot_confusion_matrix(cm, model_name, ax):
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False,
                annot_kws={"size": 24, "weight": "bold"})  # Increase font size of numbers
    ax.set_xlabel('Predicted', fontsize=20, fontweight='bold')  # Increase font size
    ax.set_ylabel('Actual', fontsize=20, fontweight='bold')  # Increase font size
    ax.set_title(f'{model_name}', fontsize=22, fontweight='bold')  # Use abbreviation as title
    ax.xaxis.set_ticklabels(['Negative', 'Positive'], fontsize=18, fontweight='bold')  # Increase font size
    ax.yaxis.set_ticklabels(['Negative', 'Positive'], fontsize=18, fontweight='bold')  # Increase font size

# Function to plot ROC curves
def plot_roc_curves(results, X_test, y_test):
    plt.figure(figsize=(12, 10))
    for model_name, metrics in results.items():
        model = metrics['model']
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        auc = roc_auc_score(y_test, y_pred_proba)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})', linewidth=2)

    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random', linewidth=2)
    plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
    plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')
    plt.legend(loc="lower right", fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig('ROC_curves.png', dpi=300, bbox_inches='tight')
    plt.close()

if __name__ == '__main__':
    print("Starting to read data...")
    data = pd.read_excel('OxidativePhosphorylationDataset.xlsx')  # Modify to your dataset file name
    print("Data reading completed")

    # Data preprocessing
    features = data.iloc[:, 1:-1]
    labels = data.iloc[:, -1]

    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=42)

    # Define models and their fixed parameters
    model_params = {
        'RF': RandomForestClassifier(n_estimators=100, random_state=42),
        'GBoost': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),
        'LR': LogisticRegression(max_iter=1000, C=1, random_state=42),
        'SVM': SVC(probability=True, C=1, kernel='rbf', random_state=42)
    }

    results = {}
    performance_data = []

    # Train models and evaluate performance
    for model_name, model in model_params.items():
        print(f"Training {model_name} model...")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Calculate performance metrics
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
        sensitivity = recall_score(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)
        specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])
        precision = precision_score(y_test, y_pred)
        mcc = matthews_corrcoef(y_test, y_pred)

        # Save results
        results[model_name] = {
            "model": model,
            "accuracy": accuracy,
            "f1_score": f1,
            "auc": auc,
            "sensitivity": sensitivity,
            "specificity": specificity,
            "precision": precision,
            "mcc": mcc,
            "confusion_matrix": cm
        }

        performance_data.append({
            "Model": model_name,
            "Accuracy": accuracy,
            "F1 Score": f1,
            "AUC": auc,
            "Sensitivity": sensitivity,
            "Specificity": specificity,
            "Precision": precision,
            "MCC": mcc,
        })

        # Print performance metrics
        print(f"{model_name} Metrics: Accuracy={accuracy:.4f}, F1 Score={f1:.4f}, AUC={auc:.4f}, "  
              f"Sensitivity={sensitivity:.4f}, Specificity={specificity:.4f}, Precision={precision:.4f}, MCC={mcc:.4f}")

    # Output performance metrics to Excel
    performance_df = pd.DataFrame(performance_data)
    performance_df.to_excel("ModelPerformanceMetrics.xlsx", index=False)
    print("Model performance metrics have been saved to 'ModelPerformanceMetrics.xlsx'.")

    # Plot heatmap of confusion matrices
    fig, axes = plt.subplots(2, 2, figsize=(20, 20))
    axes = axes.flatten()

    for idx, (model_name, metrics) in enumerate(results.items()):
        cm = metrics['confusion_matrix']
        plot_confusion_matrix(cm, model_name, axes[idx])

    plt.tight_layout()
    plt.savefig('ConfusionMatrixHeatmap.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("Confusion matrix heatmap has been saved as 'ConfusionMatrixHeatmap.png'.")

    # Plot ROC curves
    plot_roc_curves(results, X_test, y_test)
    print("ROC curves have been saved as 'ROC_curves.png'.")
